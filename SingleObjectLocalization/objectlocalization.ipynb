{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "import glob\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch import optim,nn \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-24 16:15:19.724852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-24 16:15:19.856258: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-24 16:15:19.859860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/local/ZOHOCORP/kala-pt5650/Python-Files/Python3.10.4/bin/venv3.9/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-24 16:15:19.859872: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-24 16:15:19.878539: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-24 16:15:20.487322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/local/ZOHOCORP/kala-pt5650/Python-Files/Python3.10.4/bin/venv3.9/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-24 16:15:20.487416: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/local/ZOHOCORP/kala-pt5650/Python-Files/Python3.10.4/bin/venv3.9/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-11-24 16:15:20.487421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(\"objectLoc/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "class ShapeDataset(Dataset):\n",
    "    def __init__(self,path):\n",
    "        self.class_label = {\"circle\":0,\"rectangle\":1,\"triangle\":2,\"square\":3,\"star\":4,\"bg\":5}\n",
    "        self.Image_path_list = []\n",
    "\n",
    "        for data_path in glob.glob(path+\"**/*.jpg\"):\n",
    "            self.Image_path_list.append(data_path)\n",
    "       \n",
    "        np.random.shuffle(self.Image_path_list)\n",
    "        \n",
    "    def get_label_box(self,csvFileName,  image_size ):\n",
    "       \n",
    "        df = pd.read_csv(csvFileName,\n",
    "        usecols=['ClassName','X','Y','Width','Height'])\n",
    "        df = np.array(df)[0]\n",
    "        # print(df[1:].astype(np.float32))\n",
    "        label ,boxes = self.class_label[df[0]],(df[1:].astype(np.float32)/image_size)\n",
    "        boxes = torch.from_numpy(boxes)\n",
    "        return label,boxes\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.Image_path_list[index],0)\n",
    "        image_size = 224\n",
    "        img = cv2.resize(img,(image_size,  image_size ),interpolation=cv2.INTER_AREA).reshape(1,image_size,image_size)\n",
    "        img = torch.from_numpy(img)/255.0\n",
    "        one_hot_label = torch.zeros(6,dtype=torch.float32)\n",
    "        label,boxes = self.get_label_box(self.Image_path_list[index][:-4]+\".csv\",  image_size )\n",
    "        one_hot_label[label] = 1\n",
    "        return img,boxes,one_hot_label\n",
    "    def __len__(self):\n",
    "        return len(self.Image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 2500 test: 500 total: 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" Dataset initalization  \"\"\"\n",
    "\n",
    "train_path = \"ShapeData/Train\"\n",
    "test_path = \"ShapeData/Test\"\n",
    "train_dataset = ShapeDataset(train_path)\n",
    "\n",
    "test_dataset = ShapeDataset(test_path)\n",
    "print(\"train :\",len(train_dataset),\"test:\",len(test_dataset),\"total:\",len(train_dataset)+len(test_dataset))\n",
    "# plt.imshow(tarin_dataset[0][0].T)\n",
    "train_dataset[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBn(nn.Module):\n",
    "    def __init__(self,input_ch,out_ch,stride):\n",
    "        super().__init__()\n",
    "        self.Conv = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channels=input_ch,out_channels=out_ch,kernel_size=3,stride=stride,padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthWiseSeparableConv(nn.Module):\n",
    "    def __init__(self,input_ch,out_ch,stride=1):\n",
    "        super().__init__()\n",
    "        # Depthwise convolution\n",
    "        self.ConvDw = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_ch,out_channels=input_ch,kernel_size=3,stride=stride,padding=1,groups=input_ch),\n",
    "        nn.BatchNorm2d(num_features=input_ch),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "        # Pointwise convolution\n",
    "        self.ConvPw = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(in_channels=input_ch,out_channels=out_ch,kernel_size=1,stride=1),\n",
    "            nn.BatchNorm2d(num_features=out_ch),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.ConvDw(x)\n",
    "        x = self.ConvPw(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(ConvBn(1,32,1),# 224\n",
    "                    DepthWiseSeparableConv(32,64,2),# 112\n",
    "                    DepthWiseSeparableConv(64,128,2),# 56\n",
    "                    DepthWiseSeparableConv(128,128,1),# 56\n",
    "                    DepthWiseSeparableConv(128,256,2),# 28\n",
    "                    DepthWiseSeparableConv(256,256,1), # 28\n",
    "                    DepthWiseSeparableConv(256,512,2), # 14\n",
    "                    DepthWiseSeparableConv(512,512,1),# 14\n",
    "                    DepthWiseSeparableConv(512,256,2),# 7\n",
    "                    nn.AvgPool2d(7))\n",
    "\n",
    "       \n",
    "        self.fc = nn.Linear(256,256)\n",
    "        self.class_fc = nn.Linear(256,6)\n",
    "        self.box_fc = nn.Linear(256,4)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(-1,256)\n",
    "        x = self.fc(x)\n",
    "        class_x = self.class_fc(x)\n",
    "        box_x = self.box_fc(x)\n",
    "        return class_x,box_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8\n",
    "drop_last = True\n",
    "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,num_workers=4, drop_last=drop_last)\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=True,num_workers=4, drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_loss_func = nn.CrossEntropyLoss()\n",
    "boxes_loss_func = nn.MSELoss()\n",
    "# boxes_loss_func2 = nn.MSELoss()\n",
    "model = MobileNet()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,boxes,labels= next(iter(train_loader))\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('shape_images', img_grid)\n",
    "writer.add_graph(model,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxes_loss_func1 = nn.MSELoss(reduce=False)\n",
    "# boxes_loss_func2 = nn.MSELoss()\n",
    "# x1 = torch.tensor([[0.89,0.67,0.45,0.56],[0.89,0.67,0.45,0.56]])\n",
    "# x2 = torch.tensor([[0.89,0.56,0.67,0.89],[0.89,0.56,0.67,0.89]])\n",
    "# # x2 = torch.nan\n",
    "# l1 = boxes_loss_func1(x1,x2)\n",
    "# l2 = boxes_loss_func2(x1,x2)\n",
    "# l1,l2,torch.mean((x1-x2)**2)\n",
    "# # x1*x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.0\n"
     ]
    }
   ],
   "source": [
    "print_format = 'Epoch {:03}: | Train Loss: {:.5f} | Test Loss:{:.5f} | val_accuracy: {:.5f}'\n",
    "\n",
    "total_batches_for_train = len(train_dataset)/batch_size\n",
    "total_batches_for_train = np.floor(total_batches_for_train) if drop_last else np.ceil(total_batches_for_train)\n",
    "\n",
    "total_batches_for_test = len(test_dataset)/batch_size\n",
    "total_batches_for_test = np.floor(total_batches_for_test) if drop_last else np.ceil(total_batches_for_test)\n",
    "\n",
    "print(total_batches_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x_train , b_train , y_train in train_loader :\n",
    "#     print(x_train.shape,b_train.shape,y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_loader,test_dataset,class_loss_func,boxes_loss_func,epochs=100):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss,total_test_loss,correct=0,0,0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for x_train , b_train , y_train in train_loader :\n",
    "            optimizer.zero_grad()\n",
    "            z_train = model(x_train)\n",
    "            # class loss\n",
    "            train_class_loss = class_loss_func(z_train[0],y_train)\n",
    "            # box loss \n",
    "            nan = torch.isnan(b_train)\n",
    "            B_train = torch.where(nan, torch.tensor(0.0), b_train)\n",
    "            BZ_train  = torch.where(nan, torch.tensor(0.0), z_train[1])\n",
    "            train_box_loss = boxes_loss_func(BZ_train,B_train)\n",
    "\n",
    "            # total train loss\n",
    "            train_loss = train_box_loss + train_class_loss\n",
    "           \n",
    "            #backprop\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss +=train_loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        for x_test, b_test , y_test in test_loader:\n",
    "            z_test = model(x_test)\n",
    "            test_class_loss = class_loss_func(z_test[0],y_test)\n",
    "\n",
    "            nan = torch.isnan(b_test)\n",
    "            B_test = torch.where(nan, torch.tensor(0.0), b_test)\n",
    "            # BZ_test = torch.where(nan, torch.tensor(0.0), z_test[1])\n",
    "            test_box_loss = boxes_loss_func(z_test[1],B_test)\n",
    "\n",
    "            test_loss = test_class_loss + test_box_loss \n",
    "\n",
    "            _,z_hat = torch.max(z_test[0],1)\n",
    "            _,y_hat = torch.max(y_test,1)\n",
    "            correct += (z_hat == y_hat).sum().item()\n",
    "            \n",
    "            total_test_loss += test_loss.item()\n",
    "\n",
    "        total_test_loss = total_test_loss/total_batches_for_test\n",
    "        total_train_loss = total_train_loss/total_batches_for_train\n",
    "        val_accuracy = (correct / len(test_dataset)) * 100\n",
    "        total_train_loss = total_train_loss/total_batches_for_test\n",
    "\n",
    "        writer.add_scalar(f\"Train/Train-loss\",total_train_loss,epoch)\n",
    "        writer.add_scalar(f\"Acc/Accuracy\",val_accuracy,epoch)\n",
    "        writer.add_scalar(f\"Train/Test-loss\",total_test_loss,epoch)\n",
    "\n",
    "        print(print_format.format(epoch,total_train_loss,total_test_loss,val_accuracy),end=\"\\n\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: | Train Loss: 0.011 | Test Loss:0.428 | val_accuracy: 83.00000\n",
      "Epoch 001: | Train Loss: 0.003 | Test Loss:3.843 | val_accuracy: 28.40000\n",
      "Epoch 002: | Train Loss: 0.001 | Test Loss:0.321 | val_accuracy: 86.40000\n",
      "Epoch 003: | Train Loss: 0.002 | Test Loss:0.071 | val_accuracy: 97.20000\n",
      "Epoch 004: | Train Loss: 0.001 | Test Loss:0.094 | val_accuracy: 97.00000\n",
      "Epoch 005: | Train Loss: 0.001 | Test Loss:0.054 | val_accuracy: 98.20000\n",
      "Epoch 006: | Train Loss: 0.001 | Test Loss:0.146 | val_accuracy: 94.60000\n",
      "Epoch 007: | Train Loss: 0.001 | Test Loss:0.054 | val_accuracy: 97.40000\n",
      "Epoch 008: | Train Loss: 0.000 | Test Loss:0.073 | val_accuracy: 97.20000\n",
      "Epoch 009: | Train Loss: 0.001 | Test Loss:0.063 | val_accuracy: 97.80000\n",
      "Epoch 010: | Train Loss: 0.001 | Test Loss:0.018 | val_accuracy: 99.00000\n",
      "Epoch 011: | Train Loss: 0.000 | Test Loss:0.047 | val_accuracy: 98.00000\n",
      "Epoch 012: | Train Loss: 0.000 | Test Loss:0.088 | val_accuracy: 96.60000\n",
      "Epoch 013: | Train Loss: 0.001 | Test Loss:0.085 | val_accuracy: 98.00000\n",
      "Epoch 014: | Train Loss: 0.000 | Test Loss:0.052 | val_accuracy: 98.00000\n",
      "Epoch 015: | Train Loss: 0.000 | Test Loss:0.048 | val_accuracy: 97.80000\n",
      "Epoch 016: | Train Loss: 0.000 | Test Loss:0.040 | val_accuracy: 97.40000\n",
      "Epoch 017: | Train Loss: 0.001 | Test Loss:0.132 | val_accuracy: 95.20000\n",
      "Epoch 018: | Train Loss: 0.000 | Test Loss:0.242 | val_accuracy: 91.80000\n",
      "Epoch 019: | Train Loss: 0.000 | Test Loss:0.064 | val_accuracy: 97.00000\n",
      "Epoch 020: | Train Loss: 0.000 | Test Loss:0.191 | val_accuracy: 93.80000\n",
      "Epoch 021: | Train Loss: 0.002 | Test Loss:2.199 | val_accuracy: 61.00000\n",
      "Epoch 022: | Train Loss: 0.000 | Test Loss:0.046 | val_accuracy: 98.20000\n",
      "Epoch 023: | Train Loss: 0.000 | Test Loss:0.052 | val_accuracy: 98.40000\n",
      "Epoch 024: | Train Loss: 0.000 | Test Loss:0.080 | val_accuracy: 97.20000\n",
      "Epoch 025: | Train Loss: 0.000 | Test Loss:0.054 | val_accuracy: 97.00000\n",
      "Epoch 026: | Train Loss: 0.000 | Test Loss:0.044 | val_accuracy: 98.20000\n",
      "Epoch 027: | Train Loss: 0.000 | Test Loss:0.061 | val_accuracy: 98.20000\n",
      "Epoch 028: | Train Loss: 0.000 | Test Loss:0.180 | val_accuracy: 94.00000\n",
      "Epoch 029: | Train Loss: 0.001 | Test Loss:0.029 | val_accuracy: 98.40000\n",
      "Epoch 030: | Train Loss: 0.000 | Test Loss:14.299 | val_accuracy: 23.80000\n",
      "Epoch 031: | Train Loss: 0.001 | Test Loss:0.018 | val_accuracy: 99.00000\n",
      "Epoch 032: | Train Loss: 0.000 | Test Loss:0.017 | val_accuracy: 99.00000\n",
      "Epoch 033: | Train Loss: 0.000 | Test Loss:0.027 | val_accuracy: 98.40000\n",
      "Epoch 034: | Train Loss: 0.000 | Test Loss:0.024 | val_accuracy: 98.00000\n",
      "Epoch 035: | Train Loss: 0.000 | Test Loss:0.026 | val_accuracy: 98.20000\n",
      "Epoch 036: | Train Loss: 0.000 | Test Loss:0.029 | val_accuracy: 98.20000\n",
      "Epoch 037: | Train Loss: 0.000 | Test Loss:0.010 | val_accuracy: 98.80000\n",
      "Epoch 038: | Train Loss: 0.000 | Test Loss:0.038 | val_accuracy: 98.80000\n",
      "Epoch 039: | Train Loss: 0.000 | Test Loss:0.019 | val_accuracy: 98.80000\n",
      "Epoch 040: | Train Loss: 0.000 | Test Loss:0.016 | val_accuracy: 98.80000\n",
      "Epoch 041: | Train Loss: 0.000 | Test Loss:0.017 | val_accuracy: 99.00000\n",
      "Epoch 042: | Train Loss: 0.000 | Test Loss:0.059 | val_accuracy: 96.40000\n",
      "Epoch 043: | Train Loss: 0.000 | Test Loss:4.910 | val_accuracy: 41.60000\n",
      "Epoch 044: | Train Loss: 0.000 | Test Loss:0.290 | val_accuracy: 92.00000\n",
      "Epoch 045: | Train Loss: 0.000 | Test Loss:0.150 | val_accuracy: 94.20000\n",
      "Epoch 046: | Train Loss: 0.000 | Test Loss:0.485 | val_accuracy: 85.00000\n",
      "Epoch 047: | Train Loss: 0.000 | Test Loss:0.297 | val_accuracy: 89.20000\n",
      "Epoch 048: | Train Loss: 0.000 | Test Loss:0.224 | val_accuracy: 91.00000\n",
      "Epoch 049: | Train Loss: 0.000 | Test Loss:0.193 | val_accuracy: 92.00000\n",
      "Epoch 050: | Train Loss: 0.000 | Test Loss:0.035 | val_accuracy: 98.20000\n",
      "Epoch 051: | Train Loss: 0.000 | Test Loss:0.050 | val_accuracy: 98.80000\n",
      "Epoch 052: | Train Loss: 0.000 | Test Loss:0.044 | val_accuracy: 98.80000\n",
      "Epoch 053: | Train Loss: 0.000 | Test Loss:0.088 | val_accuracy: 97.00000\n",
      "Epoch 054: | Train Loss: 0.000 | Test Loss:0.117 | val_accuracy: 96.20000\n",
      "Epoch 055: | Train Loss: 0.001 | Test Loss:0.054 | val_accuracy: 97.60000\n",
      "Epoch 056: | Train Loss: 0.000 | Test Loss:0.039 | val_accuracy: 98.40000\n",
      "Epoch 057: | Train Loss: 0.000 | Test Loss:0.319 | val_accuracy: 88.40000\n",
      "Epoch 058: | Train Loss: 0.000 | Test Loss:1.541 | val_accuracy: 61.20000\n",
      "Epoch 059: | Train Loss: 0.000 | Test Loss:0.854 | val_accuracy: 75.80000\n",
      "Epoch 060: | Train Loss: 0.000 | Test Loss:0.222 | val_accuracy: 90.60000\n",
      "Epoch 061: | Train Loss: 0.000 | Test Loss:0.172 | val_accuracy: 93.60000\n",
      "Epoch 062: | Train Loss: 0.000 | Test Loss:0.166 | val_accuracy: 94.20000\n",
      "Epoch 063: | Train Loss: 0.000 | Test Loss:0.142 | val_accuracy: 95.00000\n",
      "Epoch 064: | Train Loss: 0.000 | Test Loss:0.050 | val_accuracy: 98.40000\n",
      "Epoch 065: | Train Loss: 0.000 | Test Loss:0.327 | val_accuracy: 87.40000\n",
      "Epoch 066: | Train Loss: 0.000 | Test Loss:1.957 | val_accuracy: 55.20000\n",
      "Epoch 067: | Train Loss: 0.000 | Test Loss:0.071 | val_accuracy: 96.80000\n",
      "Epoch 068: | Train Loss: 0.000 | Test Loss:0.095 | val_accuracy: 96.00000\n",
      "Epoch 069: | Train Loss: 0.000 | Test Loss:0.034 | val_accuracy: 98.40000\n",
      "Epoch 070: | Train Loss: 0.000 | Test Loss:0.078 | val_accuracy: 97.40000\n",
      "Epoch 071: | Train Loss: 0.000 | Test Loss:0.045 | val_accuracy: 98.60000\n",
      "Epoch 072: | Train Loss: 0.000 | Test Loss:0.040 | val_accuracy: 98.60000\n",
      "Epoch 073: | Train Loss: 0.000 | Test Loss:0.078 | val_accuracy: 97.60000\n",
      "Epoch 074: | Train Loss: 0.000 | Test Loss:0.038 | val_accuracy: 98.20000\n",
      "Epoch 075: | Train Loss: 0.000 | Test Loss:0.048 | val_accuracy: 97.80000\n",
      "Epoch 076: | Train Loss: 0.000 | Test Loss:0.066 | val_accuracy: 97.60000\n",
      "Epoch 077: | Train Loss: 0.000 | Test Loss:0.054 | val_accuracy: 97.60000\n",
      "Epoch 078: | Train Loss: 0.000 | Test Loss:0.469 | val_accuracy: 82.80000\n",
      "Epoch 079: | Train Loss: 0.000 | Test Loss:0.050 | val_accuracy: 98.00000\n",
      "Epoch 080: | Train Loss: 0.000 | Test Loss:0.037 | val_accuracy: 98.20000\n",
      "Epoch 081: | Train Loss: 0.000 | Test Loss:0.028 | val_accuracy: 98.40000\n",
      "Epoch 082: | Train Loss: 0.000 | Test Loss:0.030 | val_accuracy: 98.20000\n",
      "Epoch 083: | Train Loss: 0.000 | Test Loss:0.034 | val_accuracy: 98.20000\n",
      "Epoch 084: | Train Loss: 0.000 | Test Loss:0.052 | val_accuracy: 97.60000\n",
      "Epoch 085: | Train Loss: 0.000 | Test Loss:0.031 | val_accuracy: 98.20000\n",
      "Epoch 086: | Train Loss: 0.000 | Test Loss:0.308 | val_accuracy: 94.40000\n",
      "Epoch 087: | Train Loss: 0.000 | Test Loss:0.024 | val_accuracy: 98.80000\n",
      "Epoch 088: | Train Loss: 0.000 | Test Loss:0.374 | val_accuracy: 92.80000\n",
      "Epoch 089: | Train Loss: 0.000 | Test Loss:2.676 | val_accuracy: 56.60000\n",
      "Epoch 090: | Train Loss: 0.000 | Test Loss:0.140 | val_accuracy: 95.00000\n",
      "Epoch 091: | Train Loss: 0.000 | Test Loss:0.025 | val_accuracy: 98.60000\n",
      "Epoch 092: | Train Loss: 0.000 | Test Loss:0.027 | val_accuracy: 98.60000\n",
      "Epoch 093: | Train Loss: 0.000 | Test Loss:0.049 | val_accuracy: 98.20000\n",
      "Epoch 094: | Train Loss: 0.000 | Test Loss:0.041 | val_accuracy: 98.60000\n",
      "Epoch 095: | Train Loss: 0.000 | Test Loss:0.028 | val_accuracy: 98.80000\n",
      "Epoch 096: | Train Loss: 0.000 | Test Loss:0.031 | val_accuracy: 98.80000\n",
      "Epoch 097: | Train Loss: 0.000 | Test Loss:0.073 | val_accuracy: 98.40000\n",
      "Epoch 098: | Train Loss: 0.001 | Test Loss:0.276 | val_accuracy: 93.80000\n",
      "Epoch 099: | Train Loss: 0.000 | Test Loss:0.115 | val_accuracy: 98.40000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train(model,train_loader,test_dataset,class_loss_func,boxes_loss_func)\n\u001b[0;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapeModels/objloc_weights.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "trained_model = train(model,train_loader,test_dataset,class_loss_func,boxes_loss_func)\n",
    "torch.save(trained_model.state_dict(),\"ShapeModels/objloc_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85608488f3a17fdaa1b364b9e22bf12e969be5eafa99dc98ce11e328ed13206b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
